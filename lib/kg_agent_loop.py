from __future__ import annotations

import json
import re
import time
import uuid
from dataclasses import dataclass
from typing import Any

from google import genai
from google.genai import types

from lib.kg_hybrid_graph_rag import kg_hybrid_graph_rag_with_bills as kg_hybrid_graph_rag
from lib.utils.config import config


def _should_trace() -> bool:
    """Check if chat tracing is enabled."""
    return getattr(config, "chat_trace", False)


def _start_timer() -> float:
    """Start a timer and return the start time."""
    return time.perf_counter()


def _end_timer(start_time: float) -> float:
    """End a timer and return elapsed seconds."""
    return time.perf_counter() - start_time


def _format_duration(seconds: float) -> str:
    """Format duration as ms or seconds with 2 decimal places."""
    if seconds < 1:
        return f"{seconds * 1000:.0f}ms"
    return f"{seconds:.2f}s"


def _format_node(node: dict[str, Any]) -> str:
    """Format a single KG node for trace logging."""
    node_id = node.get("id", "?")
    node_label = node.get("label", "?")[:30]
    node_type = node.get("type", "?")
    return f"{node_id} ({node_type}): {node_label}"


def _format_edge(edge: dict[str, Any]) -> str:
    """Format a single KG edge for trace logging."""
    edge_id = edge.get("id", "?")[:20]
    pred = edge.get("predicate", "?")[:30]
    source = edge.get("source_label", "?")[:20]
    target = edge.get("target_label", "?")[:20]
    return f"{edge_id}: {source} --[{pred}]--> {target}"


def _format_citation(citation: dict[str, Any]) -> str:
    """Format a single citation for trace logging."""
    uid = citation.get("utterance_id", "?")
    speaker = citation.get("speaker_name", "?")[:20]
    text_preview = (citation.get("text", "") or "")[:60]
    return f"{uid}: {speaker} said '{text_preview}...'"


def _format_tool_result_summary(result: dict[str, Any]) -> str:
    """Format a kg_hybrid_graph_rag result for trace logging."""
    out: list[str] = []
    out.append(f"query='{result.get('query', '?')[:50]}...'")
    out.append(f"hops={result.get('hops', '?')}")
    out.append(f"seeds_count={len(result.get('seeds', []))}")
    out.append(f"nodes_count={len(result.get('nodes', []))}")
    out.append(f"edges_count={len(result.get('edges', []))}")
    out.append(f"citations_count={len(result.get('citations', []))}")

    if result.get("nodes") and len(result["nodes"]) <= 5:
        node_labels = [f"{n.get('label', '?')[:20]}" for n in result["nodes"][:3]]
        out.append(f"nodes_preview={node_labels}")

    if result.get("citations") and len(result["citations"]) <= 5:
        cite_ids = [f"{c.get('utterance_id', '?')}" for c in result["citations"][:3]]
        out.append(f"citations_preview={cite_ids}")

    return ", ".join(out)


def _truncate_text(text: str, max_len: int = 300) -> str:
    """Truncate text to max_len with ellipsis."""
    if not text or len(text) <= max_len:
        return text or ""
    return text[: max_len - 3] + "..."


def _format_content_part_summary(part: types.Part) -> dict[str, Any]:
    """Format a content part for trace logging."""
    if part.text:
        return {"type": "text", "preview": _truncate_text(part.text, 300)}
    if part.function_call:
        fc = part.function_call
        return {
            "type": "function_call",
            "name": getattr(fc, "name", ""),
            "args_keys": list(dict(fc.args or {}).keys()) if fc.args else [],
        }
    if part.function_response:
        return {"type": "function_response", "name": part.function_response.name or ""}
    return {"type": "unknown"}


def _format_contents_summary(contents: list[types.Content]) -> list[dict[str, Any]]:
    """Format contents list for trace logging."""
    if not contents:
        return []
    result: list[dict[str, Any]] = []
    for c in contents:
        parts_list = getattr(c, "parts", None) or []
        parts_summary = [_format_content_part_summary(p) for p in parts_list]
        result.append({"role": c.role or "unknown", "parts": parts_summary})
    return result


def _serialize_content_part(part: types.Part) -> dict[str, Any]:
    """Serialize a content part to dict for raw logging."""
    if part.text:
        return {"type": "text", "content": part.text}
    if part.function_call:
        fc = part.function_call
        return {
            "type": "function_call",
            "name": getattr(fc, "name", ""),
            "args": dict(fc.args) if fc.args else {},
        }
    if part.function_response:
        return {
            "type": "function_response",
            "name": part.function_response.name or "",
            "response": part.function_response.response,
        }
    return {"type": "unknown"}


def _serialize_contents(contents: list[types.Content]) -> list[dict[str, Any]]:
    """Serialize contents list for raw logging."""
    if not contents:
        return []
    result: list[dict[str, Any]] = []
    for c in contents:
        parts_list = getattr(c, "parts", None) or []
        parts_serialized = [_serialize_content_part(p) for p in parts_list]
        result.append({"role": c.role or "unknown", "parts": parts_serialized})
    return result


def _trace_print(trace_id: str, section: str, message: str) -> None:
    """Print a trace message with consistent formatting."""
    if not _should_trace():
        return
    print(f"üîç [TRACE {trace_id}] {section}")
    print(f"  {message}")


def _trace_section_start(trace_id: str, section: str) -> None:
    """Print a trace section header."""
    if not _should_trace():
        return
    print(f"\n{'=' * 60}")
    print(f"üîç [TRACE {trace_id}] {section}")
    print(f"{'=' * 60}")


def _trace_section_end(trace_id: str) -> None:
    """Print a trace section footer."""
    if not _should_trace():
        return
    print(f"{'=' * 60}\n")


AGENT_RESPONSE_SCHEMA: dict[str, Any] = {
    "type": "object",
    "properties": {
        "answer": {"type": "string"},
        "cite_utterance_ids": {"type": "array", "items": {"type": "string"}},
        "focus_node_ids": {"type": "array", "items": {"type": "string"}},
        "followup_questions": {"type": "array", "items": {"type": "string"}},
    },
    "required": ["answer", "cite_utterance_ids", "focus_node_ids"],
}


def _parse_json_best_effort(text: str | None) -> dict[str, Any] | None:
    if not text:
        return None
    raw = text.strip()
    if raw.startswith("```json"):
        raw = raw[7:]
    elif raw.startswith("```"):
        raw = raw[3:]
    if raw.endswith("```"):
        raw = raw[:-3]
    raw = raw.strip()
    try:
        return json.loads(raw)
    except Exception:
        return None


_LEADING_INTERJECTION_RE = re.compile(r"^(wuh\s*-?\s*loss)[\s,!.:-]+", re.IGNORECASE)
_KEY_CONNECTIONS_BLOCK_RE = re.compile(
    r"(?ims)^\s*#{0,6}\s*key connections\s*$\n(?:^\s*[-*]\s.*\n)+\n?",
)


def _promote_section_headings(text: str) -> str:
    lines = text.splitlines()
    out: list[str] = []

    def is_blank(s: str) -> bool:
        return not s.strip()

    for i, line in enumerate(lines):
        stripped = line.strip()
        prev_blank = i == 0 or is_blank(lines[i - 1])
        next_nonblank = i + 1 < len(lines) and not is_blank(lines[i + 1])

        if (
            prev_blank
            and next_nonblank
            and stripped
            and not stripped.startswith("#")
            and not stripped.startswith(">")
            and not stripped.startswith("-")
            and not stripped.endswith(":")
            and len(stripped) <= 64
            and re.fullmatch(r"[A-Z][A-Za-z0-9 \"'\-()]+", stripped) is not None
        ):
            # Heuristic: if it's mostly Title Case words, treat as a section heading.
            words = [w for w in re.split(r"\s+", stripped) if w]
            small = {
                "and",
                "or",
                "the",
                "a",
                "an",
                "of",
                "to",
                "for",
                "in",
                "on",
                "with",
            }
            titleish = 0
            for w in words:
                lw = w.lower().strip("\"'()")
                if lw in small:
                    titleish += 1
                elif w[:1].isupper():
                    titleish += 1
            if len(words) >= 2 and (titleish / max(1, len(words))) >= 0.75:
                out.append(f"### {stripped}")
                continue

        out.append(line)

    return "\n".join(out)


def _clean_answer_text(text: str | None) -> str:
    raw = (text or "").strip()
    if not raw:
        return ""
    raw = _LEADING_INTERJECTION_RE.sub("", raw).lstrip()
    raw = _KEY_CONNECTIONS_BLOCK_RE.sub("", raw).strip()
    raw = _promote_section_headings(raw)
    raw, _ = _extract_embedded_followup_questions(raw)
    return raw


def _extract_embedded_followup_questions(text: str) -> tuple[str, list[str]]:
    lines = text.splitlines()
    marker_idx = -1
    for i, line in enumerate(lines):
        if re.search(r"follow\s*-?\s*up\s+questions", line, re.IGNORECASE):
            marker_idx = i
            break

    if marker_idx < 0:
        return text, []

    tail_lines = [ln.strip() for ln in lines[marker_idx + 1 :] if ln.strip() and ln.strip() != "-"]
    questionish = sum(1 for ln in tail_lines if ln.endswith("?"))

    # Only strip when this clearly looks like a generated follow-up section.
    if questionish < 2:
        return text, []

    followups = [re.sub(r"^[-*\d.)\s]+", "", ln).strip() for ln in tail_lines if ln.endswith("?")]
    followups = [q for q in followups if q]

    trimmed = "\n".join(lines[:marker_idx]).rstrip()
    return trimmed, followups


def _infer_citation_ids_from_bracket_numbers(
    answer: str, retrieval: dict[str, Any] | None
) -> list[str]:
    if not isinstance(retrieval, dict):
        return []

    citations = [
        c
        for c in (retrieval.get("citations") or [])
        if isinstance(c, dict) and str(c.get("utterance_id") or "").strip()
    ]
    if not citations:
        return []

    # Only infer plain numeric markers like "... [3] ...".
    # Ignore markdown links such as "[3](#src:utt_123)".
    indices = [int(m.group(1)) for m in re.finditer(r"\[(\d+)\](?!\s*\()", answer or "")]
    out: list[str] = []
    seen: set[str] = set()
    for idx in indices:
        if idx <= 0 or idx > len(citations):
            continue
        uid = str(citations[idx - 1].get("utterance_id") or "").strip()
        if not uid or uid in seen:
            continue
        seen.add(uid)
        out.append(uid)
    return out


def _normalize_citation_id(raw_id: str) -> str:
    raw = str(raw_id or "").strip()
    raw = re.sub(r"^https?://[^#]+#", "", raw, flags=re.IGNORECASE)
    raw = re.sub(r"^#?src:", "", raw, flags=re.IGNORECASE)
    raw = re.sub(r"^source:", "", raw, flags=re.IGNORECASE)
    raw = re.sub(r"[\]\),.;]+$", "", raw)
    return raw.strip()


def _citation_lookup_keys(citation_id: str) -> list[str]:
    normalized = _normalize_citation_id(citation_id)
    if not normalized:
        return []

    keys = [normalized, normalized.lower()]
    if normalized.startswith("utt_"):
        bare = normalized[4:]
        if bare:
            keys.extend([bare, bare.lower()])
    else:
        keys.extend([f"utt_{normalized}", f"utt_{normalized.lower()}"])

    out: list[str] = []
    seen: set[str] = set()
    for key in keys:
        if not key or key in seen:
            continue
        seen.add(key)
        out.append(key)
    return out


def _infer_citation_ids_from_src_links(answer: str, retrieval: dict[str, Any] | None) -> list[str]:
    src_tokens = [
        m.group(1).strip()
        for m in re.finditer(r"(?:#src:|source:)([^,\s)\]]+)", answer or "", re.IGNORECASE)
    ]
    if not src_tokens:
        return []

    known_lookup: dict[str, str] = {}
    known_ids: list[str] = []
    if isinstance(retrieval, dict):
        for citation in retrieval.get("citations") or []:
            if not isinstance(citation, dict):
                continue
            known_id = str(citation.get("utterance_id") or "").strip()
            if not known_id:
                continue
            known_ids.append(known_id)
            for key in _citation_lookup_keys(known_id):
                known_lookup[key] = known_id

    suffix_counts: dict[str, int] = {}
    for known_id in known_ids:
        match = re.search(r":(\d+)$", known_id)
        if not match:
            continue
        seconds = match.group(1)
        suffix_counts[seconds] = suffix_counts.get(seconds, 0) + 1

    out: list[str] = []
    seen: set[str] = set()
    for token in src_tokens:
        normalized = _normalize_citation_id(token)
        if not normalized:
            continue

        resolved = normalized
        if known_lookup:
            matched = None
            for key in _citation_lookup_keys(normalized):
                if key in known_lookup:
                    matched = known_lookup[key]
                    break
            if matched is None:
                sec_match = re.match(r"^(?:utt_)?(\d+)$", normalized, re.IGNORECASE)
                if sec_match:
                    seconds = sec_match.group(1)
                    if suffix_counts.get(seconds) == 1:
                        matched = next(
                            (
                                known_id
                                for known_id in known_ids
                                if known_id.endswith(f":{seconds}")
                            ),
                            None,
                        )
            if matched is None:
                continue
            resolved = matched

        if resolved in seen:
            continue
        seen.add(resolved)
        out.append(resolved)
    return out


def _filter_to_known_citation_ids(
    cite_utterance_ids: list[str],
    retrieval: dict[str, Any] | None,
) -> list[str]:
    if not cite_utterance_ids:
        return []
    if not isinstance(retrieval, dict):
        out: list[str] = []
        seen: set[str] = set()
        for raw in cite_utterance_ids:
            uid = _normalize_citation_id(str(raw or ""))
            key = uid.lower()
            if not uid or key in seen:
                continue
            seen.add(key)
            out.append(uid)
        return out

    known_ids = [
        str(c.get("utterance_id") or "").strip()
        for c in (retrieval.get("citations") or [])
        if isinstance(c, dict)
    ]
    known_ids = [k for k in known_ids if k]
    if not known_ids:
        return [
            _normalize_citation_id(str(x or ""))
            for x in cite_utterance_ids
            if _normalize_citation_id(str(x or ""))
        ]

    known_lookup: dict[str, str] = {}
    for known_id in known_ids:
        for key in _citation_lookup_keys(known_id):
            known_lookup[key] = known_id

    suffix_counts: dict[str, int] = {}
    for known_id in known_ids:
        match = re.search(r":(\d+)$", known_id)
        if not match:
            continue
        seconds = match.group(1)
        suffix_counts[seconds] = suffix_counts.get(seconds, 0) + 1

    out: list[str] = []
    seen: set[str] = set()
    for raw in cite_utterance_ids:
        uid = _normalize_citation_id(str(raw or ""))
        if not uid:
            continue

        resolved = None
        for key in _citation_lookup_keys(uid):
            if key in known_lookup:
                resolved = known_lookup[key]
                break

        if resolved is None:
            sec_match = re.match(r"^(?:utt_)?(\d+)$", uid, re.IGNORECASE)
            if sec_match:
                seconds = sec_match.group(1)
                if suffix_counts.get(seconds) == 1:
                    resolved = next(
                        (known_id for known_id in known_ids if known_id.endswith(f":{seconds}")),
                        None,
                    )

        if not resolved or resolved in seen:
            continue
        seen.add(resolved)
        out.append(resolved)
    return out


@dataclass
class _ToolCall:
    name: str
    args: dict[str, Any]


class KGAgentLoop:
    def __init__(
        self,
        *,
        postgres: Any,
        embedding_client: Any,
        client: Any | None = None,
        model: str = "gemini-3-flash-preview",
        max_tool_iterations: int = 4,
        enable_thinking: bool = False,
    ) -> None:
        self.postgres = postgres
        self.embedding_client = embedding_client
        self.model = model
        self.max_tool_iterations = max_tool_iterations
        self.enable_thinking = enable_thinking

        if client is not None:
            self.client = client
        else:
            import os

            api_key = os.getenv("GOOGLE_API_KEY")
            if not api_key:
                raise ValueError("GOOGLE_API_KEY environment variable is not set")
            self.client = genai.Client(api_key=api_key)

    def _system_prompt(self) -> str:
        return (
            "You are YuhHearDem, a friendly AI guide to Barbados Parliament debates. "
            "Keep a lightly Caribbean (Bajan) tone - warm and plainspoken, not cheesy.\n\n"
            "You MUST ground everything in retrieved evidence from knowledge graph and transcript utterances.\n\n"
            "Rules:\n"
            "- Before answering, call the tool `kg_hybrid_graph_rag` to retrieve a compact subgraph + citations, including bill excerpts for legislation questions.\n"
            "- Construct the `query` parameter using CONCISE KEYWORDS separated by spaces, NOT natural language questions.\n"
            "  Examples:\n"
            '    ‚úÖ "Barbados Water Authority BWA water scarcity drought infrastructure"\n'
            '    ‚úÖ "housing development christ church vesting resolution"\n'
            '    ‚úÖ "Road Traffic Bill amendment penalties clause"\n'
            '    ‚ùå "What did ministers say about water management?"\n'
            "- Include key entity names and important concepts (3-10 terms max, keep under 15 words).\n"
            "- Use ONLY the tool results as your source of truth. Do not invent facts.\n"
            "- Interpret the tool results: use `edges` + `nodes` + `bill_citations` to explain relationships (who/what connects to what, agreements/disagreements, proposals, responsibilities) in plain language.\n"
            "- For questions about bills, legislation, or clauses, prefer citing bill excerpts from `bill_citations` when available using format `[cite](#src:bill:<bill_id>:<chunk_index>)`.\n"
            "- Prefer quoting MPs directly when citations are available; use markdown blockquotes and put the quoted sentence in *italics*.\n"
            "- Add visible inline citations in the answer body using markdown links like `[1](#src:utt_123)` or `[cite](#src:utt_123)` for transcript citations, or `[cite](#src:bill:<bill_id>:<chunk_index>)` for bill citations.\n"
            "- Use only utterance_ids that appear in tool citations.\n"
            "- Use short section headings for main themes using markdown like `### The Climate Change Clash`.\n"
            "- Do NOT include a section called 'Key connections' and do NOT show technical arrow notation like `A -> PREDICATE -> B`.\n"
            "- Do NOT start your answer with filler like 'Wuhloss,'; start directly with the point.\n"
            "- Your `answer` field may contain markdown (bullets, bold, blockquotes).\n"
            "- When you make a claim, include at least one citation by listing its `utterance_id` in `cite_utterance_ids`.\n"
            "- If evidence is insufficient, say so clearly and ask one precise follow-up.\n"
            "- Generate 2-4 follow-up questions based on your answer that users might naturally want to ask next. These should be specific questions that explore related topics, dive deeper into mentioned entities, or ask about related legislation or debates.\n"
            "- Return JSON only matching the response schema."
        )

    def _tool_declarations(self) -> list[types.Tool]:
        tool_schema = {
            "type": "object",
            "properties": {
                "query": {"type": "string"},
                "hops": {"type": "integer", "default": 1},
                "seed_k": {"type": "integer", "default": 12},
                "max_edges": {"type": "integer", "default": 90},
                "max_citations": {"type": "integer", "default": 12},
                "max_bill_citations": {"type": "integer", "default": 8},
                "edge_rank_threshold": {"type": "number"},
            },
            "required": ["query"],
        }
        fd = types.FunctionDeclaration(
            name="kg_hybrid_graph_rag",
            description=(
                "Hybrid Graph-RAG: vector/fulltext seed search over kg_nodes, then expand kg_edges N hops "
                "and return a compact subgraph plus provenance citations with youtube timecoded URLs. "
                "Also retrieves bill excerpt citations for legislation questions. "
                "Use edge_rank_threshold to filter low-quality edges (0.05 recommended after normalization)."
            ),
            parameters_json_schema=tool_schema,
        )
        return [types.Tool(function_declarations=[fd])]

    def _messages_to_contents(
        self, history: list[dict[str, str]], user_message: str
    ) -> list[types.Content]:
        contents: list[types.Content] = []
        for msg in history:
            role = msg.get("role")
            content = msg.get("content") or ""
            if not content:
                continue
            gemini_role = "user" if role == "user" else "model"
            contents.append(
                types.Content(role=gemini_role, parts=[types.Part.from_text(text=content)])
            )

        contents.append(types.Content(role="user", parts=[types.Part.from_text(text=user_message)]))
        return contents

    def _extract_function_calls(self, response: Any) -> list[_ToolCall]:
        fcs = getattr(response, "function_calls", None)
        if not fcs:
            return []
        out: list[_ToolCall] = []
        for fc in fcs:
            name = getattr(fc, "name", None)
            args = getattr(fc, "args", None)
            if not name:
                continue
            out.append(_ToolCall(name=str(name), args=dict(args or {})))
        return out

    async def _call_llm(self, contents: list[types.Content], is_tool_call: bool) -> Any:
        config_params: dict[str, Any] = {
            "system_instruction": self._system_prompt(),
            "temperature": 0.2,
            "max_output_tokens": 2048,
        }
        if is_tool_call:
            config_params["tools"] = self._tool_declarations()
        else:
            # Final answer call should be structured JSON. Keep tools disabled here
            # because Gemini rejects response_mime_type JSON when function calling is enabled.
            config_params["response_schema"] = AGENT_RESPONSE_SCHEMA
            config_params["response_mime_type"] = "application/json"
        if not self.enable_thinking:
            config_params["thinking_config"] = types.ThinkingConfig(thinking_budget=0)
        config = types.GenerateContentConfig(**config_params)
        return await self.client.aio.models.generate_content(
            model=self.model,
            contents=contents,
            config=config,
        )

    async def run(self, *, user_message: str, history: list[dict[str, str]]) -> dict[str, Any]:
        trace_id = str(uuid.uuid4())[:8]
        total_start = _start_timer()

        _trace_section_start(trace_id, "KG AGENT LOOP START")
        _trace_print(trace_id, "User Query", _truncate_text(user_message, 200))
        _trace_print(
            trace_id,
            "History",
            f"{len(history)} messages (last {min(3, len(history))} shown if tracing enabled)",
        )
        _trace_section_end(trace_id)

        contents = self._messages_to_contents(history, user_message)
        last_retrieval: dict[str, Any] | None = None

        _trace_section_start(trace_id, "ITERATION 0 - LLM CALL")
        _trace_print(trace_id, "Context Summary", f"{len(contents)} content parts")
        if _should_trace():
            for i, c in enumerate(_format_contents_summary(contents)):
                print(f"    [{i}] {c['role']}: {c['parts']}")
            print(f"\nüîç [TRACE {trace_id}] RAW CONTENTS SENT TO LLM")
            serialized = _serialize_contents(contents)
            for i, c in enumerate(serialized):
                print(f"\n  [{i}] Role: {c['role']}")
                for j, p in enumerate(c.get("parts", [])):
                    p_type = p.get("type", "unknown")
                    print(f"    [{j}] Type: {p_type}")
                    if p_type == "text":
                        print(f"        Content: {p.get('content', '')}")
                    elif p_type == "function_call":
                        print(f"        Name: {p.get('name', '')}")
                        print(f"        Args: {p.get('args', {})}")
                    elif p_type == "function_response":
                        print(f"        Name: {p.get('name', '')}")
                        print(f"        Response type: {type(p.get('response', {})).__name__}")
            print(f"\n{'=' * 60}\n")
        llm_start = _start_timer()
        response = await self._call_llm(contents, is_tool_call=True)
        llm_duration = _end_timer(llm_start)
        _trace_print(trace_id, "Duration", _format_duration(llm_duration))
        if _should_trace():
            response_text = getattr(response, "text", None) or ""
            print(f"\nüîç [TRACE {trace_id}] RAW LLM RESPONSE")
            print(f"  Length: {len(response_text)} chars")
            print(f"  Content:\n{response_text}")
            print(f"\n{'=' * 60}\n")
        _trace_section_end(trace_id)

        iterations = 0

        while True:
            _trace_section_start(trace_id, f"PARSING LLM RESPONSE (iteration {iterations})")

            candidates = getattr(response, "candidates", None)
            if candidates:
                cand0 = candidates[0]
                model_content = getattr(cand0, "content", None)
                if model_content is not None:
                    contents.append(model_content)

            function_calls = self._extract_function_calls(response)

            if not function_calls:
                _trace_print(trace_id, "Function Calls", "None - loop complete")
                _trace_section_end(trace_id)
                break

            _trace_print(
                trace_id,
                "Function Calls",
                f"{len(function_calls)} call(s): {[fc.name for fc in function_calls]}",
            )

            _trace_section_end(trace_id)

            iterations += 1
            if iterations > self.max_tool_iterations:
                _trace_print(
                    trace_id,
                    "Limit",
                    f"Max tool iterations ({self.max_tool_iterations}) reached",
                )
                break

            _trace_section_start(trace_id, f"EXECUTING TOOLS (iteration {iterations})")
            result_parts: list[types.Part] = []
            for fc in function_calls:
                if fc.name == "kg_hybrid_graph_rag":
                    _trace_print(
                        trace_id,
                        "Tool Call",
                        f"kg_hybrid_graph_rag(query={_truncate_text(str(fc.args.get('query', '')), 100)}, hops={fc.args.get('hops', 1)}, seed_k={fc.args.get('seed_k', 12)}, threshold={fc.args.get('edge_rank_threshold')})",
                    )
                    tool_start = _start_timer()
                    tool_result = kg_hybrid_graph_rag(
                        postgres=self.postgres,
                        embedding_client=self.embedding_client,
                        query=str(fc.args.get("query", "")),
                        hops=int(fc.args.get("hops", 1)),
                        seed_k=int(fc.args.get("seed_k", 12)),
                        max_edges=int(fc.args.get("max_edges", 90)),
                        max_citations=int(fc.args.get("max_citations", 12)),
                        max_bill_citations=int(fc.args.get("max_bill_citations", 8)),
                        edge_rank_threshold=float(fc.args.get("edge_rank_threshold"))
                        if fc.args.get("edge_rank_threshold") is not None
                        else None,
                    )
                    tool_duration = _end_timer(tool_start)
                    last_retrieval = tool_result

                    if _should_trace():
                        _trace_print(
                            trace_id,
                            "Tool Duration",
                            _format_duration(tool_duration),
                        )
                        _trace_print(
                            trace_id,
                            "Tool Result",
                            _format_tool_result_summary(tool_result),
                        )

                    result_parts.append(
                        types.Part.from_function_response(
                            name=fc.name,
                            response=tool_result,
                        )
                    )
                else:
                    _trace_print(
                        trace_id,
                        "Tool Error",
                        f"unknown tool: {fc.name}",
                    )
                    result_parts.append(
                        types.Part.from_function_response(
                            name=fc.name,
                            response={"error": f"unknown tool: {fc.name}"},
                        )
                    )

            contents.append(types.Content(role="user", parts=result_parts))
            if _should_trace() and result_parts:
                _trace_print(
                    trace_id,
                    "Tool Response to LLM",
                    f"function_response(name='kg_hybrid_graph_rag') with {len(result_parts)} part(s)",
                )
                response_summary = (
                    result_parts[0].function_response.response
                    if result_parts[0].function_response
                    else {}
                )
                if isinstance(response_summary, dict):
                    for key in ["query", "hops", "nodes", "edges", "citations"]:
                        val = response_summary.get(key)
                        if isinstance(val, list):
                            _trace_print(trace_id, f"  {key}_count", f"{len(val)}")
                        elif val is not None:
                            _trace_print(trace_id, f"  {key}", str(val)[:100])

                    print(f"\nüîç [TRACE {trace_id}] ACTUAL KG DATA SENT TO LLM")
                    nodes = response_summary.get("nodes", [])[:5]
                    edges = response_summary.get("edges", [])[:5]
                    citations = response_summary.get("citations", [])[:5]

                    if nodes:
                        print(
                            f"  Nodes (first {len(nodes)} of {len(response_summary.get('nodes', []))}):"
                        )
                        for i, n in enumerate(nodes):
                            print(f"    [{i}] {_format_node(n)}")

                    if edges:
                        print(
                            f"\n  Edges (first {len(edges)} of {len(response_summary.get('edges', []))}):"
                        )
                        for i, e in enumerate(edges):
                            print(f"    [{i}] {_format_edge(e)}")

                    if citations:
                        print(
                            f"\n  Citations (first {len(citations)} of {len(response_summary.get('citations', []))}):"
                        )
                        for i, c in enumerate(citations):
                            print(f"    [{i}] {_format_citation(c)}")

                    print(f"\n{'=' * 60}\n")

            _trace_section_end(trace_id)

            _trace_section_start(trace_id, f"ITERATION {iterations} - LLM CALL")
            _trace_print(trace_id, "Context Summary", f"{len(contents)} content parts")
            if _should_trace():
                for i, c in enumerate(_format_contents_summary(contents)):
                    print(f"    [{i}] {c['role']}: {len(c['parts'])} part(s)")
                    for j, p in enumerate(c["parts"][:2]):
                        p_type = p.get("type", "unknown")
                        preview = p.get("preview", "")
                        if preview:
                            print(f"        [{j}] {p_type}: {preview}")
                print(f"\nüîç [TRACE {trace_id}] RAW CONTENTS SENT TO LLM")
                serialized = _serialize_contents(contents)
                for i, c in enumerate(serialized):
                    print(f"\n  [{i}] Role: {c['role']}")
                    for j, p in enumerate(c.get("parts", [])):
                        p_type = p.get("type", "unknown")
                        print(f"    [{j}] Type: {p_type}")
                        if p_type == "text":
                            print(f"        Content: {p.get('content', '')}")
                        elif p_type == "function_call":
                            print(f"        Name: {p.get('name', '')}")
                            print(f"        Args: {p.get('args', {})}")
                        elif p_type == "function_response":
                            print(f"        Name: {p.get('name', '')}")
                            print(f"        Response type: {type(p.get('response', {})).__name__}")
                print(f"\n{'=' * 60}\n")
            llm_start = _start_timer()
            response = await self._call_llm(contents, is_tool_call=False)
            llm_duration = _end_timer(llm_start)
            _trace_print(trace_id, "Duration", _format_duration(llm_duration))
            if _should_trace():
                response_text = getattr(response, "text", None) or ""
                print(f"\nüîç [TRACE {trace_id}] RAW LLM RESPONSE")
                print(f"  Length: {len(response_text)} chars")
                print(f"  Content:\n{response_text}")
                print(f"\n{'=' * 60}\n")
            _trace_section_end(trace_id)

        _trace_section_start(trace_id, "FINAL ANSWER PARSING")
        parsed = _parse_json_best_effort(getattr(response, "text", None))
        if not parsed:
            parsed = {
                "answer": getattr(response, "text", None) or "I couldn't generate an answer.",
                "cite_utterance_ids": [],
                "focus_node_ids": [],
                "followup_questions": [],
            }

        parsed.setdefault("cite_utterance_ids", [])
        parsed.setdefault("focus_node_ids", [])
        parsed.setdefault("answer", "")
        parsed.setdefault("followup_questions", [])

        parsed["followup_questions"] = [
            str(q).strip()
            for q in list(parsed.get("followup_questions") or [])
            if str(q or "").strip()
        ][:4]

        cleaned_answer, embedded_followups = _extract_embedded_followup_questions(
            str(parsed.get("answer") or "")
        )
        if embedded_followups:
            merged_followups = list(parsed.get("followup_questions") or []) + embedded_followups
            deduped: list[str] = []
            seen_followups: set[str] = set()
            for q in merged_followups:
                normalized_q = str(q).strip()
                if not normalized_q or normalized_q in seen_followups:
                    continue
                seen_followups.add(normalized_q)
                deduped.append(normalized_q)
            parsed["followup_questions"] = deduped[:4]
        parsed["answer"] = cleaned_answer

        cite_ids = _filter_to_known_citation_ids(
            list(parsed.get("cite_utterance_ids") or []),
            last_retrieval,
        )
        inferred_ids = _infer_citation_ids_from_bracket_numbers(
            parsed.get("answer", ""), last_retrieval
        )
        inferred_ids += _infer_citation_ids_from_src_links(parsed.get("answer", ""), last_retrieval)
        for inferred in inferred_ids:
            if inferred not in cite_ids:
                cite_ids.append(inferred)
        parsed["cite_utterance_ids"] = cite_ids

        parsed["answer"] = _clean_answer_text(parsed.get("answer"))
        parsed["retrieval"] = last_retrieval

        total_duration = _end_timer(total_start)
        if _should_trace():
            _trace_print(
                trace_id,
                "Final Answer Summary",
                f"length={len(parsed.get('answer', ''))} chars, "
                f"cite_ids={parsed.get('cite_utterance_ids', [])[:3]}{'...' if len(parsed.get('cite_utterance_ids', [])) > 3 else ''}, "
                f"focus_nodes={parsed.get('focus_node_ids', [])[:3]}{'...' if len(parsed.get('focus_node_ids', [])) > 3 else ''}",
            )
            _trace_print(trace_id, "Total Duration", _format_duration(total_duration))
            _trace_section_end(trace_id)

        return parsed
